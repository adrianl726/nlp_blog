[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! Hope you like it here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adrian’s Blog",
    "section": "",
    "text": "A Guide to Encoding Texts for Natural Language Processing\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nAdrian Leung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nlp/index.html",
    "href": "posts/nlp/index.html",
    "title": "A Guide to Encoding Texts for Natural Language Processing",
    "section": "",
    "text": "Source: Google"
  },
  {
    "objectID": "posts/nlp/index.html#introduction",
    "href": "posts/nlp/index.html#introduction",
    "title": "A Guide to Encoding Texts for Natural Language Processing",
    "section": "Introduction",
    "text": "Introduction\nNatural Language Processing (NLP) is a fascinating field of Machine Learning that focuses on enabling machines to understand, interpret, and generate human language. From facilitating translation tools like Google Translate to powering voice assistance like Siri and Alexa, NLP’s influence on the current technological landascape is indisputable and substantial. With the ever growing interest and expanding development in artificial intelligence, a lot of aspiring engineers and scientists are looking to venture into the lucrative NLP field. However, before we learn to perform all the fancy tasks such as text generations and summarizations, we must start from the fundamentals and ask ourselves a question:\nHow can we bridge the gap between human communication and machine processing?\nOne problem arises from this question is that computers do not understand language the way humans do. They are not wired to comprehend words and write essays like we do. Instead, they operate on numbers. All the NLP models are driven by mathematical algorithms and formulae. Thus, encoding text into numerical representations becomes the key to computers learning human language. By converting words, sentences, or entire documents into numbers, NLP models can perform a wide range of tasks like analyzing patterns, extracting meanings, and generating responses.\nThis blog will introduce and guide you through different methods and tools to encode texts, thus providing you a gateway to using NLP models."
  },
  {
    "objectID": "posts/nlp/index.html#challenges",
    "href": "posts/nlp/index.html#challenges",
    "title": "A Guide to Encoding Texts for Natural Language Processing",
    "section": "Challenges",
    "text": "Challenges\nBefore we learn about different ways to encode texts, we need to acknowledge the challenges associated with the intricacy of human language. Language is messy and unpredictable. Although there are sets of grammatical rules that govern a language, humans are prone to making mistakes yet still able to convey their messages. For example, “How is you doin” is grammatically incorrect but we know it means “How are you doing”. Thus, language is not strictly restricted by an algorithm, contrary to how computers operate.\nMoreover, not all words have meanings. Auxiliary verbs like “is” and “am” do not contribute or change the message a sentence wants to convey. They are meaningless outside of abiding by grammatical rules. There is also a hierarchy of meanings in a sentence. Certain words can mean more than others. Consider the sentence “We are happy”. Although “we” and “happy” play their roles in conveying our emotions, “happy” is a more important word as it tells the key emotion.\nTo complicate matters more, a word can have different meanings depending on the context. Even more confusingly, some words can have completely opposite meanings. For example, the word “left” in the sentence “We just left” means departed. However, it means staying when the sentence is “We are the only one left”. This shows that contexts can alter meanings of the same word drastically.\n\n\n\n\nContext matters!!! (Source: Kamala Harris)\n\n\n\nHence, making computers comprehend language like we do is far from a simple task. Encoding words with numerical representations is a work of art as it determines how well a model can understand us."
  },
  {
    "objectID": "posts/nlp/index.html#approaches-to-text-encoding",
    "href": "posts/nlp/index.html#approaches-to-text-encoding",
    "title": "A Guide to Encoding Texts for Natural Language Processing",
    "section": "Approaches to Text Encoding",
    "text": "Approaches to Text Encoding\nThis section will cover different approaches to encoding texts including traditional methods like Bag-of-Words (BoW) and TF-IDF, word-level embeddings, contextualized embeddings, and sentence-level and document-level representations.\n\nTraditional Methods\n\nBag-of-Words (BoW)\nBoW is one of the most popular encoding methods. It encodes each unique word from all input documents with a number based on their count or presence in their respective document.\nConsider the example below:\nUnique words in all documents: [‘the’, ‘bird’, ‘is’, ‘cat’, ‘and’, ‘dog’, ‘hate’, ‘each’, ‘no’, ‘other’]\nAnd we pick one of the documents for our first BoW representation as below.\nDocument: “The cat and the cat hate each other.”\nIn the case of encoding each word with its count, the BoW model will transform the document to the representation in Table 1. Since we have two ‘the’ and ‘cat’ in the document above, the numerical representations for ‘the’ and ‘cat’ in this document are 2.\n\n\n\n\n\n\nthe\nbird\nis\ncat\nand\ndog\nhate\neach\nno\nother\n\n\n\n\n2\n0\n0\n2\n1\n0\n1\n1\n0\n1\n\n\n\n\n\nTable 1: BoW representations using word counts\n\n\n\nAnd in the case of measuring each word by its presence as seen in Table 2, BoW uses binary values 0 and 1 to represent each word, where 0 implies absence and 1 implies presence. Note that the words ‘the’ and ‘cat’ are represented by 1 instead of 2 since we are using binary representations.\n\n\n\n\n\n\nthe\nbird\nis\ncat\nand\ndog\nhate\neach\nno\nother\n\n\n\n\n1\n0\n0\n1\n1\n0\n1\n1\n0\n1\n\n\n\n\n\nTable 2: BoW representations using binary indicators"
  },
  {
    "objectID": "posts/nlp/index.html#text-encoding-approaches",
    "href": "posts/nlp/index.html#text-encoding-approaches",
    "title": "A Guide to Encoding Texts for Natural Language Processing",
    "section": "Text Encoding Approaches",
    "text": "Text Encoding Approaches\nThis section will cover different approaches to encoding texts including traditional methods like Bag-of-Words (BoW) and TF-IDF, word embeddings, contextualized embeddings, and sentence-level and document-level representations.\n\nTraditional Methods\n\nBag-of-Words (BoW)\nBoW is one of the most popular encoding methods. It encodes each unique word from all input documents with a number based on their count or presence in their respective document.\nConsider the example below:\nUnique words in all documents: [‘the’, ‘bird’, ‘is’, ‘cat’, ‘and’, ‘dog’, ‘hate’, ‘each’, ‘no’, ‘other’]\nAnd we pick one of the documents for our first BoW representation as below.\nDocument: “The cat and the cat hate each other.”\nIn the case of encoding each word with its count, the BoW model will transform the document to the representation in Table 1. Since we have two ‘the’ and ‘cat’ in the document above, the numerical representations for ‘the’ and ‘cat’ in this document are 2.\n\n\n\n\n\n\nthe\nbird\nis\ncat\nand\ndog\nhate\neach\nno\nother\n\n\n\n\n2\n0\n0\n2\n1\n0\n1\n1\n0\n1\n\n\n\n\n\nTable 1: BoW representations using word counts\n\n\n\nAnd in the case of measuring each word by its presence as seen in Table 2, BoW uses binary values 0 and 1 to represent each word, where 0 implies absence and 1 implies presence. Note that the words ‘the’ and ‘cat’ are represented by 1 instead of 2 since we are using binary representations.\n\n\n\n\n\n\nthe\nbird\nis\ncat\nand\ndog\nhate\neach\nno\nother\n\n\n\n\n1\n0\n0\n1\n1\n0\n1\n1\n0\n1\n\n\n\n\n\nTable 2: BoW representations using binary indicators\n\n\n\nTo extract BoW representations in Python [1], we can leverage the CountVectorizer function from the scikit-learn package [2]. Table 3 demonstrates using CountVectorizer for BoW extraction by word count with the same example.\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Using above example\ndocuments = [\n    'The cat and the cat hate each other.',\n    'The dog is the bird.',\n    'No, the bird and cat hate other dog.'\n]\n\nbow = CountVectorizer()\nX = bow.fit_transform(documents)\nbow_df = pd.DataFrame(\n    X.toarray(), columns=bow.get_feature_names_out(), index=documents\n)\nbow_df\n\n\n\n\n\n\n\n\n\n\n\nand\nbird\ncat\ndog\neach\nhate\nis\nno\nother\nthe\n\n\n\n\nThe cat and the cat hate each other.\n1\n0\n2\n0\n1\n1\n0\n0\n1\n2\n\n\nThe dog is the bird.\n0\n1\n0\n1\n0\n0\n1\n0\n0\n2\n\n\nNo, the bird and cat hate other dog.\n1\n1\n1\n1\n0\n1\n0\n1\n1\n1\n\n\n\n\n\n\n\n\nTable 3: BoW results from CountVectorizer\n\n\n\n\nAlthough the BoW method is as intuitive and self-explanatory as it seems, it is far from a perfect model as it discards the word order in the original document. It disregards how words can form meaningful word phrases and change their meanings with respect to the context.\n\n\nTF-IDF\nTF-IDF, which stands for term frequency-inverse document frequency, is another popular method to encode text. It is a measure of relevance of a word in a document. The computation can be broken down into two parts: term frequency and inverse document frequency.\nTerm Frequency (TF)\nThe term frequency is the count of a given word \\(w\\) in a given document \\(d\\), divided by the total number of words in document \\(d\\).\n\\[TF = \\frac{\\text{Number of word $w$ in document $d$}}{\\text{Total number of words in document $d$}}\\]\nInverse document frequency (IDF)\nThe inverse document frequency is to penalize words that are too common across all documents. For example, auxiliary verbs like ‘is’ are weighed less as the result. In return, this gives rise to rarer words that possibly carry more meaning and importance.\n\\[IDF = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing word $w$}}\\right)\\]\nCombining both TF and IDF\nTo have our TF-IDF representation, we multiply both terms to have the following formula:\n\\[TF\\text{-}IDF = TF \\times IDF\\]\nNow, let’s revisit our example with TF-IDF in Python [1]. Luckily, the scikit-learn package [2] also has a function for TF-IDF called TfidfVectorizer. As shown in the first row in Table 4, the TF-IDF representation for ‘the’ is smaller than ‘cat’ even though they both have the same BoW representations in Table 3. This is the result of the compensation from IDF as ‘the’ appears too much across documents.\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Using the same example\ndocuments = [\n    'The cat and the cat hate each other.',\n    'The dog is the bird.',\n    'No, the bird and cat hate other dog.'\n]\n\ntfidf = TfidfVectorizer()\nX = tfidf.fit_transform(documents)\ntfidf_df = pd.DataFrame(\n    X.toarray(), columns=tfidf.get_feature_names_out().tolist(), index=documents\n)\ntfidf_df\n\n\n\n\n\n\n\n\n\n\n\nand\nbird\ncat\ndog\neach\nhate\nis\nno\nother\nthe\n\n\n\n\nThe cat and the cat hate each other.\n0.299594\n0.000000\n0.599187\n0.000000\n0.39393\n0.299594\n0.000000\n0.000000\n0.299594\n0.465322\n\n\nThe dog is the bird.\n0.000000\n0.403525\n0.000000\n0.403525\n0.00000\n0.000000\n0.530587\n0.000000\n0.000000\n0.626747\n\n\nNo, the bird and cat hate other dog.\n0.346438\n0.346438\n0.346438\n0.346438\n0.00000\n0.346438\n0.000000\n0.455524\n0.346438\n0.269040\n\n\n\n\n\n\n\n\nTable 4: TF-IDF results from TfidfVectorizer\n\n\n\n\nTF-IDF is a step-up from BoW as it recognizes what makes a word important in a document. However, similar to BoW, it also disregards the order and context of words. Thus, we need some alternatives that can compute an even better representation for words.\n\n\n\nWord Embeddings\nThis is where word embeddings come into play. Contrary to traditional methods, word embeddings encode words in vector forms (from Linear Algebra!). Through vector representations, they can encapsulate the relationships between words by showing their similarity numerically. Mathematically, the similarity in words are measured by how close the embeddings are in the vector space. Figure 1 shows an example of visualizing word embeddings in a 2-dimensional space. As you can see, words that are similar in meaning or context are clustered together. This is the art of word embeddings!\n\n\n\n\n\n\nFigure 1: Visualization of word embeddings (Source: Ruben Winastwan)\n\n\n\n\nWord2Vec\nOne common tool to obtain word embeddings is Word2Vec [3]."
  },
  {
    "objectID": "posts/nlp/index.html#challenges-of-textual-data",
    "href": "posts/nlp/index.html#challenges-of-textual-data",
    "title": "A Guide to Encoding Texts for Natural Language Processing",
    "section": "Challenges of Textual Data",
    "text": "Challenges of Textual Data\nBefore we learn about different ways to encode texts, we need to acknowledge the challenges associated with the intricacy of human language. Language is messy and unpredictable. Although there are sets of grammatical rules that govern a language, humans are prone to making mistakes yet still able to convey their messages. For example, “How is you doin” is grammatically incorrect but we know it means “How are you doing”. Thus, language is not strictly restricted by an algorithm, contrary to how computers operate.\nMoreover, not all words have meanings. Auxiliary verbs like “is” and “am” do not contribute or change the message a sentence wants to convey. They are meaningless outside of abiding by grammatical rules. There is also a hierarchy of meanings in a sentence. Certain words can mean more than others. Consider the sentence “We are happy”. Although “we” and “happy” play their roles in conveying our emotions, “happy” is a more important word as it tells the key emotion.\nTo complicate matters more, a word can have different meanings depending on the context. Even more confusingly, some words can have completely opposite meanings. For example, the word “left” in the sentence “We just left” means departed. However, it means staying when the sentence is “We are the only one left”. This shows that contexts can alter meanings of the same word drastically.\n\n\n\n\nContext matters!!! (Source: Kamala Harris)\n\n\n\nHence, making computers comprehend language like we do is far from a simple task. Encoding words with numerical representations is a work of art as it determines how well a model can understand us."
  },
  {
    "objectID": "posts/nlp/index.html#approaches-to-encoding-texts",
    "href": "posts/nlp/index.html#approaches-to-encoding-texts",
    "title": "A Guide to Encoding Texts for Natural Language Processing",
    "section": "Approaches to Encoding Texts",
    "text": "Approaches to Encoding Texts\nThis section will cover different approaches to encoding texts including traditional methods like Bag-of-Words (BoW) and TF-IDF, word embeddings, and contextualized embeddings.\n\nTraditional Methods\n\nBag-of-Words (BoW)\nBoW is one of the most popular encoding methods. It encodes each unique word from all input documents with a number based on their count or presence in their respective document.\nConsider the example below:\nUnique words in all documents: [‘the’, ‘bird’, ‘is’, ‘cat’, ‘and’, ‘dog’, ‘hate’, ‘each’, ‘no’, ‘other’]\nAnd we pick one of the documents for our first BoW representation as below.\nDocument: “The cat and the cat hate each other.”\nIn the case of encoding each word with its count, the BoW model will transform the document to the representation in Table 1. Since we have two ‘the’ and ‘cat’ in the document above, the numerical representations for ‘the’ and ‘cat’ in this document are 2.\n\n\n\n\n\n\nthe\nbird\nis\ncat\nand\ndog\nhate\neach\nno\nother\n\n\n\n\n2\n0\n0\n2\n1\n0\n1\n1\n0\n1\n\n\n\n\n\nTable 1: BoW representations using word counts\n\n\n\nAnd in the case of measuring each word by its presence as seen in Table 2, BoW uses binary values 0 and 1 to represent each word, where 0 implies absence and 1 implies presence. Note that the words ‘the’ and ‘cat’ are represented by 1 instead of 2 since we are using binary representations.\n\n\n\n\n\n\nthe\nbird\nis\ncat\nand\ndog\nhate\neach\nno\nother\n\n\n\n\n1\n0\n0\n1\n1\n0\n1\n1\n0\n1\n\n\n\n\n\nTable 2: BoW representations using binary indicators\n\n\n\nTo extract BoW representations in Python [1], we can leverage the CountVectorizer function from the scikit-learn package [2]. Table 3 demonstrates using CountVectorizer for BoW extraction by word count with the same example.\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Using above example\ndocuments = [\n    'The cat and the cat hate each other.',\n    'The dog is the bird.',\n    'No, the bird and cat hate other dog.'\n]\n\nbow = CountVectorizer()\nX = bow.fit_transform(documents)\nbow_df = pd.DataFrame(\n    X.toarray(), columns=bow.get_feature_names_out(), index=documents\n)\nbow_df\n\n\n\n\n\n\n\n\n\n\n\nand\nbird\ncat\ndog\neach\nhate\nis\nno\nother\nthe\n\n\n\n\nThe cat and the cat hate each other.\n1\n0\n2\n0\n1\n1\n0\n0\n1\n2\n\n\nThe dog is the bird.\n0\n1\n0\n1\n0\n0\n1\n0\n0\n2\n\n\nNo, the bird and cat hate other dog.\n1\n1\n1\n1\n0\n1\n0\n1\n1\n1\n\n\n\n\n\n\n\n\nTable 3: BoW results from CountVectorizer\n\n\n\n\nAlthough the BoW method is as intuitive and self-explanatory as it seems, it is far from a perfect model as it discards the word order in the original document. It disregards how words can form meaningful word phrases and change their meanings with respect to the context.\n\n\nTF-IDF\nTF-IDF, which stands for term frequency-inverse document frequency, is another popular method to encode text. It is a measure of relevance of a word in a document. The computation can be broken down into two parts: term frequency and inverse document frequency.\nTerm Frequency (TF)\nThe term frequency is the count of a given word \\(w\\) in a given document \\(d\\), divided by the total number of words in document \\(d\\).\n\\[TF = \\frac{\\text{Number of word $w$ in document $d$}}{\\text{Total number of words in document $d$}}\\]\nInverse document frequency (IDF)\nThe inverse document frequency is to penalize words that are too common across all documents. For example, auxiliary verbs like ‘is’ are weighed less as the result. In return, this gives rise to rarer words that possibly carry more meaning and importance.\n\\[IDF = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing word $w$}}\\right)\\]\nCombining both TF and IDF\nTo have our TF-IDF representation, we multiply both terms to have the following formula:\n\\[TF\\text{-}IDF = TF \\times IDF\\]\nNow, let’s revisit our example with TF-IDF in Python [1]. Luckily, the scikit-learn package [2] also has a function for TF-IDF called TfidfVectorizer. As shown in the first row in Table 4, the TF-IDF representation for ‘the’ is smaller than ‘cat’ even though they both have the same BoW representations in Table 3. This is the result of the compensation from IDF as ‘the’ appears too much across documents.\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Using the same example\ndocuments = [\n    'The cat and the cat hate each other.',\n    'The dog is the bird.',\n    'No, the bird and cat hate other dog.'\n]\n\ntfidf = TfidfVectorizer()\nX = tfidf.fit_transform(documents)\ntfidf_df = pd.DataFrame(\n    X.toarray(), columns=tfidf.get_feature_names_out().tolist(), index=documents\n)\ntfidf_df\n\n\n\n\n\n\n\n\n\n\n\nand\nbird\ncat\ndog\neach\nhate\nis\nno\nother\nthe\n\n\n\n\nThe cat and the cat hate each other.\n0.299594\n0.000000\n0.599187\n0.000000\n0.39393\n0.299594\n0.000000\n0.000000\n0.299594\n0.465322\n\n\nThe dog is the bird.\n0.000000\n0.403525\n0.000000\n0.403525\n0.00000\n0.000000\n0.530587\n0.000000\n0.000000\n0.626747\n\n\nNo, the bird and cat hate other dog.\n0.346438\n0.346438\n0.346438\n0.346438\n0.00000\n0.346438\n0.000000\n0.455524\n0.346438\n0.269040\n\n\n\n\n\n\n\n\nTable 4: TF-IDF results from TfidfVectorizer\n\n\n\n\nTF-IDF is a step-up from BoW as it recognizes what makes a word important in a document. However, similar to BoW, it also disregards the order and context of words. Thus, we need some alternatives that can compute an even better representation for words.\n\n\n\nWord Embeddings\nThis is where word embeddings come into play. Contrary to traditional methods, word embeddings encode words in vector forms (from Linear Algebra!). Through vector representations, they can encapsulate the relationships between words by showing their similarity numerically. Mathematically, the similarity in words is measured by how close the embeddings are in the vector space. Figure 1 shows an example of visualizing word embeddings in a 2-dimensional space. As you can see, words that are similar in meaning or context are clustered together. This is the art of word embeddings!\n\n\n\n\n\n\nFigure 1: Visualization of word embeddings (Source: Ruben Winastwan)\n\n\n\n\nWord2Vec\nOne common tool to obtain word embeddings is Word2Vec [3]. It computes the embeddings by leveraging the architecture of two-layer neural networks. There are two approaches that Word2Vec uses to obtain these embeddings.\nContinuous BoW (CBoW)\nCBoW is a prediction algorithm where the neural network aims to predict a target word based on the existing context in a document. Simply put, this is analogous to filling in the blank in a sentence.\nConsider the sentence “My cute puppy is barking”. The model will iterate over this sentence and remove one word from each iteration. For example, as shown in Figure 2, the model omits the word ‘puppy’ from the sentence and trains the neural network to guess the word ‘puppy’ from the remaining sentence.\n\n\n\n\n\n\n\nFigure 2: Illustration of CBoW algorithm\n\n\n\n\nSkipgram\nSkipgram is the complete opposite of CBoW. Instead of predicting the missing word from a given context, skipgram predicts the surrounding context from a given word. Using the same example, as shown in Figure 3, the model will try to guess the surrounding words to the word ‘puppy’.\n\n\n\n\n\n\n\nFigure 3: Illustration of Skipgram algorithm\n\n\n\n\nAfter multiple iterations in the training process, Word2Vec will use the learned weights in the neural network from either of the approaches to construct the word embeddings for each word.\nApplying Word2Vec in Python [1] is made possible with the package Gensim [4]. As seen in Listing 1, Gensim‘s Word2Vec takes in a list of lists to generate the word embeddings. Note that the sg argument in the function lets you choose between CBoW and skipgram, where 0 and 1 corresponds to CBoW and skipgram respectively. The argument min_count tells the model to ignore words that have a fewer count than this minimum. After training Word2Vec on our sample text, we can take a quick look into how word embeddings look like for the word ’technology’ in Listing 1.\n\n\n\n\nListing 1: This code demonstrates extracting word embedding from the word ‘technology’\n\n\nimport pandas as pd\nfrom gensim.models import Word2Vec\n# Generated by ChatGPT\nsample_text = [\n    \"The advancement of technology has transformed the way we communicate and interact with the world.\",\n    \"Artificial intelligence is increasingly being used in healthcare, education, and other industries to enhance efficiency.\",\n    \"People often gather in coffee shops to discuss ideas, share stories, and enjoy a sense of community.\",\n    \"Self-driving cars and smart home devices are examples of how technology is becoming a part of our everyday lives.\",\n    \"Art galleries and cultural festivals are popular spots for people to explore creativity and connect with others.\",\n    \"The integration of AI in the workplace has sparked debates about its impact on jobs and productivity.\",\n    \"Reading books and attending literary events remain cherished activities in the digital age.\",\n    \"Many cities are blending technology with traditional practices to create unique and thriving environments.\",\n    \"The use of virtual reality in gaming and training has opened new possibilities for immersive experiences.\",\n    \"Social media platforms have changed the way we form relationships and share information globally.\"\n]\n# This generates a list of lists\nsample_sentences = [sent.split() for sent in sample_text]\n\nw2v = Word2Vec(sample_sentences, min_count = 1, sg = 1)\nprint(w2v.wv['technology'])\n\n\n\n\n[ 8.1346482e-03 -4.3696621e-03 -1.0951435e-03  1.0827162e-03\n -1.6076697e-04  1.0314664e-03  6.1621480e-03  1.0513653e-04\n -3.3200562e-03 -1.6226495e-03  5.8745840e-03  1.3788766e-03\n -6.8573974e-04  9.4051417e-03 -4.8837205e-03 -9.1557507e-04\n  9.1908397e-03  6.7008133e-03  1.4975395e-03 -9.0892995e-03\n  1.2369623e-03 -2.2766890e-03  9.4154952e-03  1.1043868e-03\n  1.5170779e-03  2.3703994e-03 -1.9285623e-03 -4.9641491e-03\n  1.0452364e-04 -2.0255600e-03  6.6222828e-03  8.9008864e-03\n -5.9560389e-04  2.8281522e-03 -6.1490987e-03  1.7546985e-03\n -6.8589435e-03 -8.6309118e-03 -5.9207552e-03 -9.0170074e-03\n  7.2529181e-03 -5.8431132e-03  8.1692915e-03 -7.1991798e-03\n  3.4998127e-03  9.6219182e-03 -7.8216838e-03 -9.9756923e-03\n -4.2250603e-03 -2.6117193e-03 -2.6378274e-04 -8.8602239e-03\n -8.5995235e-03  2.7603817e-03 -8.2284901e-03 -9.0225162e-03\n -2.3512202e-03 -8.6695738e-03 -7.1790209e-03 -8.3399629e-03\n -2.7452479e-04 -4.5728176e-03  6.6562551e-03  1.5371529e-03\n -3.3772779e-03  6.1904443e-03 -5.9688864e-03 -4.5542065e-03\n -7.3182448e-03 -4.1977805e-03 -1.7964148e-03  6.5716160e-03\n -2.7138642e-03  4.9592862e-03  6.9808913e-03 -7.4309111e-03\n  4.5860992e-03  6.1515258e-03 -2.9300000e-03  6.6001783e-03\n  6.0655614e-03 -6.4467844e-03 -6.8669897e-03  2.5851957e-03\n -1.7241427e-03 -6.1018453e-03  9.5864786e-03 -5.1106275e-03\n -6.4399107e-03 -4.0861694e-05 -2.5958647e-03  5.0307182e-04\n -3.4884498e-03 -3.8938067e-04 -6.8298483e-04  8.8520558e-04\n  8.1980843e-03 -5.7321084e-03 -1.6760164e-03  5.5565243e-03]\n\n\nAs it turns out, the word embedding for ‘technology’ is a high-dimensional vector. However, the values are much more ambiguous than the previous representations we have learned earlier. Let’s try to translate these embeddings into more comprehensible results.\nAs mentioned before, word embeddings are powerful at capturing similarities in words. We are putting this to test in Listing 2, where we ask Word2Vec what the most similar word to ‘technology’ is. The result shows that ‘advancement’ is the closest word choice. This is a reasonable pick since we often use the phrase ‘technological advancement’ when describing new technological milestones.\n\n\n\n\nListing 2: This code shows what Word2Vec thinks the most similar word to ‘technology’ is.\n\n\nprint(w2v.wv.most_similar('technology')[0])\n\n\n\n\n('advancement', 0.3503554165363312)\n\n\nWord2Vec is a big improvement from traditional methods. Nevertheless, it still has flaws such as failing to recognize unknown words. Since Word2Vec is a pre-trained model, its linguistic knowledge mainly bases on the data corpus it was trained on. Unfortunately, language changes over time as new words continue to pop up in dictionaries every year. Thus, it is only a matter of time that Word2Vec will become outdated. Another limitation of Word2Vec is that it struggles to differentiate between words with multiple meanings. Since Word2Vec generates a word embedding for each unique word, it fails to acknowledge that a word can carry separate meanings. This can be problematic especially when words can have exact opposite meanings given different contexts.\n\n\n\nContextualized Embeddings\nSimilar to word embeddings, contextualized embeddings also encode texts into high-dimensional vectors. They build on top of the existing framework of word embeddings by conditioning each word on its context. When obtaining the contextualized embeddings of a word, it includes neighbouring words in the calculation. As such, same words that appear in different contexts will have different embeddings. This provides a solution to the limitations of word embeddings as it learns the word based on the context surrounding it.\nWhile there are several architectures, such as ELMo and GPT-2, that are trained on obtaining contextualized embeddings, we will be focusing on learning the BERT model in this section.\n\n\n\nBert from Sesame Street. Coincidentally, the popular contextualized embeddings are conveniently named as Sesame Street characters (Source: Sesame Street)\n\n\n\nBERT\nBERT [5], which stands for Bidirectional encoder representations from transformers, is a transformer-based encoder model developed in 2018 by researchers at Google [6]. It is bidirectional in the sense that it captures the both left and right contexts of a given word. For example, consider the sentence “You exist in the context of all in which you live” and the target word “context”. The bidirectional nature of BERT"
  }
]